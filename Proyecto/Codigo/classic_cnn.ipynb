{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f979abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activar el entorno de conda\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e911f9d7",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "661ea2a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Could not find directory data/train",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mcache()\u001b[38;5;241m.\u001b[39mprefetch(AUTOTUNE)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Crea los tres splits a partir de carpetas separadas\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mmake_ds\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m val_ds   \u001b[38;5;241m=\u001b[39m make_ds(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/val\u001b[39m\u001b[38;5;124m\"\u001b[39m,   shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     31\u001b[0m test_ds  \u001b[38;5;241m=\u001b[39m make_ds(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/test\u001b[39m\u001b[38;5;124m\"\u001b[39m,  shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m, in \u001b[0;36mmake_ds\u001b[1;34m(path, shuffle)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_ds\u001b[39m(path, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    Carga un dataset desde una estructura de carpetas:\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m      path/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    'shuffle=True' solo para el split de entrenamiento.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_dataset_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# redimensiona cada imagen a (180,180)\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# tamaño de lote\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# barajar solo en train; en val/test mantener orden estable\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# cache(): guarda en RAM (o disco si se pasa una ruta) para evitar decodificar cada epoch\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# prefetch(): solapa el preprocesamiento con el entrenamiento\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mcache()\u001b[38;5;241m.\u001b[39mprefetch(AUTOTUNE)\n",
      "File \u001b[1;32mc:\\Users\\diego\\miniconda3\\envs\\GPUtensorflow\\lib\\site-packages\\keras\\utils\\image_dataset.py:207\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     seed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1e6\u001b[39m)\n\u001b[1;32m--> 207\u001b[0m image_paths, labels, class_names \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALLOWLIST_FORMATS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(class_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhen passing `label_mode=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`, there must be exactly 2 \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_names. Received: class_names=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\diego\\miniconda3\\envs\\GPUtensorflow\\lib\\site-packages\\keras\\utils\\dataset_utils.py:524\u001b[0m, in \u001b[0;36mindex_directory\u001b[1;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    523\u001b[0m     subdirs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 524\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[0;32m    526\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m subdir\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\diego\\miniconda3\\envs\\GPUtensorflow\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:766\u001b[0m, in \u001b[0;36mlist_directory_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of entries contained within a directory.\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \n\u001b[0;32m    753\u001b[0m \u001b[38;5;124;03mThe list is in arbitrary order. It does not contain the special entries \".\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;124;03m  errors.NotFoundError if directory doesn't exist\u001b[39;00m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_directory(path):\n\u001b[1;32m--> 766\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError(\n\u001b[0;32m    767\u001b[0m       node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    768\u001b[0m       op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    769\u001b[0m       message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find directory \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path))\n\u001b[0;32m    771\u001b[0m \u001b[38;5;66;03m# Convert each element to string, since the return values of the\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;66;03m# vector of string should be interpreted as strings, not bytes.\u001b[39;00m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    774\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_str_any(filename)\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m _pywrap_file_io\u001b[38;5;241m.\u001b[39mGetChildren(compat\u001b[38;5;241m.\u001b[39mpath_to_bytes(path))\n\u001b[0;32m    776\u001b[0m ]\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Could not find directory data/train"
     ]
    }
   ],
   "source": [
    "# Tamaño al que se redimensionarán las imágenes y tamaño de batch\n",
    "img_size = (128, 128)\n",
    "batch = 32\n",
    "\n",
    "# Ajuste automático del pipeline tf.data para solapar CPU con GPU\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def make_ds(path, shuffle=False):\n",
    "    \"\"\"\n",
    "    Carga un dataset desde una estructura de carpetas:\n",
    "      path/\n",
    "        clase_1/\n",
    "        clase_2/\n",
    "        ...\n",
    "    Redimensiona a img_size, agrupa en batches y aplica cache+prefetch para eficiencia.\n",
    "    'shuffle=True' solo para el split de entrenamiento.\n",
    "    \"\"\"\n",
    "    ds = keras.utils.image_dataset_from_directory(\n",
    "        path,\n",
    "        image_size=img_size,   # redimensiona cada imagen a (180,180)\n",
    "        batch_size=batch,      # tamaño de lote\n",
    "        shuffle=shuffle        # barajar solo en train; en val/test mantener orden estable\n",
    "    )\n",
    "    # cache(): guarda en RAM (o disco si se pasa una ruta) para evitar decodificar cada epoch\n",
    "    # prefetch(): solapa el preprocesamiento con el entrenamiento\n",
    "    return ds.cache().prefetch(AUTOTUNE)\n",
    "\n",
    "# Crea los tres splits a partir de carpetas separadas\n",
    "train_ds = make_ds(\"data/train\", shuffle=True)\n",
    "val_ds   = make_ds(\"data/val\",   shuffle=False)\n",
    "test_ds  = make_ds(\"data/test\",  shuffle=False)\n",
    "\n",
    "# Comprueba que el orden y los nombres de clases sean consistentes entre splits\n",
    "class_names = train_ds.class_names\n",
    "assert class_names == val_ds.class_names == test_ds.class_names, \"Clases no coinciden entre splits\"\n",
    "\n",
    "model = Sequential([\n",
    "    # Normaliza píxeles uint8 [0..255] -> float32 [0..1]\n",
    "    layers.Rescaling(1./255, input_shape=img_size + (3,)),\n",
    "    \n",
    "    # Bloque conv 1\n",
    "    layers.Conv2D(32, 3, padding=\"same\", kernel_initializer=\"he_uniform\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.MaxPooling2D(),\n",
    "\n",
    "    # Bloque conv 2\n",
    "    layers.Conv2D(64, 3, padding=\"same\", kernel_initializer=\"he_uniform\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.MaxPooling2D(),\n",
    "\n",
    "    # Bloque conv 3\n",
    "    layers.Conv2D(128, 3, padding=\"same\", kernel_initializer=\"he_uniform\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.MaxPooling2D(),\n",
    "\n",
    "    # GAP reduce parámetros\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    # Head denso\n",
    "    layers.Dense(256, kernel_initializer=\"he_uniform\"),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    layers.Dense(128, kernel_initializer=\"he_uniform\"),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "\n",
    "    layers.Dense(3, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# lr(t) = 0.001 * (0.9)^(step/1000)\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=False\n",
    ")\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Compila el modelo con pérdida \"sparse\" (etiquetas enteras 0..2) y accuracy como métrica\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d07586",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3a51e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrena el modelo registrando métricas en history para análisis posterior\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d90928",
   "metadata": {},
   "source": [
    "# Evaluación objetiva en el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42589403",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "print(f\"Test — loss: {test_loss:.4f}  acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389ddb69",
   "metadata": {},
   "source": [
    "# Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb3e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true: concatena las etiquetas reales de cada batch del test\n",
    "y_true = np.concatenate([y.numpy() for _, y in test_ds], axis=0)\n",
    "\n",
    "# y_pred: para cada imagen, argmax de las probabilidades softmax -> clase predicha\n",
    "y_pred = np.argmax(model.predict(test_ds), axis=1)\n",
    "\n",
    "# Genera la matriz de confusión (filas: clase real, columnas: predicción)\n",
    "cm = tf.math.confusion_matrix(y_true, y_pred, num_classes=len(class_names))\n",
    "print(\"Confusion matrix:\\n\", cm.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPUtensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
